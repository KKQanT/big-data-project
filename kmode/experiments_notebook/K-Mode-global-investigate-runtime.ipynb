{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import time\n",
    "from math import e\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "def to_numpy(row):\n",
    "  a = list(row.asDict().values())\n",
    "  return np.array(a, dtype=\"<U22\")\n",
    "\n",
    "def hamming_distance(x1, x2):\n",
    "  return np.count_nonzero(x1!=x2)\n",
    "\n",
    "def get_closest_cluster(x, centroid):\n",
    "  min_hamming_distance = np.inf\n",
    "  closest_cluster = 0\n",
    "  for i, mode in enumerate(centroid):\n",
    "     distance = hamming_distance(x, mode)\n",
    "     if distance < min_hamming_distance:\n",
    "        min_hamming_distance = distance\n",
    "        closest_cluster = i\n",
    "  return (closest_cluster, x)\n",
    "\n",
    "def get_mode_from_vec(vec):\n",
    "  counted = Counter(vec)\n",
    "  return counted.most_common(1)[0][0]\n",
    "\n",
    "def get_mode_from_arr(arr):\n",
    "  return np.apply_along_axis(get_mode_from_vec, 0, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3 #number of cluseters\n",
    "\n",
    "stop_distance = 1\n",
    "\n",
    "N_iters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCL Data\n",
    "\n",
    "uncoment the follow code to fetch UCL data <br/>\n",
    "use id = 73 for xxx data <br/>\n",
    "use id = 76 for mushroom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncoment the follow code to fetch UCL data\n",
    "### use id = 73 for xxx data\n",
    "### use id = 76 for mushroom data\n",
    "\n",
    "#ucl_data = fetch_ucirepo(id=73) \n",
    "#pdf = ucl_data.data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.read_csv('experiments_notebook/data/train.csv')\n",
    "pdf = pdf.drop(columns=['id', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"K-Mode-global\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.cache()\n",
    "\n",
    "columns = df.columns\n",
    "unique_values_dict = {}\n",
    "for col in columns:\n",
    "  unique_val_objs = df.select(col).distinct().collect()\n",
    "  unique_val_list = [row[col] for row in unique_val_objs]\n",
    "  unique_values_dict[col] = unique_val_list\n",
    "  del unique_val_list\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict = {\n",
    "    0.25: [],\n",
    "    0.5: [],\n",
    "    0.75: [],\n",
    "    1: []\n",
    "}\n",
    "\n",
    "for _ in range(1):\n",
    "\n",
    "  for (i, col) in enumerate(columns):\n",
    "    unique_values = unique_values_dict[col]\n",
    "    ramdom_vals = random.choices(unique_values, k=K)\n",
    "    if i == 0:\n",
    "      ini_centroid = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
    "    else:\n",
    "      ramdom_vals = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
    "      ini_centroid = np.hstack((ini_centroid, ramdom_vals))\n",
    "\n",
    "  for (i, col) in enumerate(columns):\n",
    "    unique_values = unique_values_dict[col]\n",
    "    ramdom_vals = random.choices(unique_values, k=K)\n",
    "    if i == 0:\n",
    "      ini_centroid = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
    "    else:\n",
    "      ramdom_vals = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
    "      ini_centroid = np.hstack((ini_centroid, ramdom_vals))\n",
    "\n",
    "  BASE_PARTITION = 400\n",
    "\n",
    "  for frac in [0.25, 0.5, 0.75, 1]:\n",
    "    print(f'-------------------------------{frac}------------------------------')\n",
    "    print(f'num partition: {int(BASE_PARTITION * frac)}')\n",
    "    df = spark.createDataFrame(pdf.sample(frac=frac).reset_index(drop=True))\n",
    "    df.cache()\n",
    "    rdd = df.rdd.repartition(int(BASE_PARTITION * frac))\n",
    "    rdd.cache()\n",
    "\n",
    "    centroid = sc.broadcast(ini_centroid)\n",
    "\n",
    "    rdd = rdd.map(lambda row: to_numpy(row))\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    for iter in range(N_iters):\n",
    "      clustered = rdd.map(lambda x: get_closest_cluster(x, centroid.value)) #-> (k, v) = (cluster_i, X)\n",
    "      group_by_clustered = clustered.reduceByKey(lambda x, y: np.vstack((x, y)))\n",
    "      centroid_rdd = group_by_clustered.map(lambda x : (x[0], get_mode_from_arr(x[1])))\n",
    "      centroid_list = centroid_rdd.collect()\n",
    "\n",
    "      new_centroid = centroid.value.copy()\n",
    "      for (i, arr) in centroid_list:\n",
    "        new_centroid[i] = arr\n",
    "\n",
    "      old_centroid = centroid.value.copy()\n",
    "      centroid = sc.broadcast(new_centroid)\n",
    "\n",
    "      distance = hamming_distance(old_centroid, new_centroid)\n",
    "\n",
    "      print('iteration : ', {iter+1}, \" hamming distance between new and previous centroid:  \", distance)\n",
    "\n",
    "      if distance <= stop_distance:\n",
    "        break\n",
    "    t_end = time.time()\n",
    "    print(f\"--- {t_end-t_start} seconds ---\")\n",
    "    runtime_dict[frac].append(t_end-t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame(runtime_dict)\n",
    "df_results.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
