{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14374,"status":"ok","timestamp":1715369338561,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"CroHpTGv-nOH","outputId":"71de9a82-11b3-4c6b-ac03-0c1103b04061"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ucimlrepo\n","  Downloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\n","Installing collected packages: ucimlrepo\n","Successfully installed ucimlrepo-0.0.6\n"]}],"source":["pip install ucimlrepo"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92839,"status":"ok","timestamp":1715369431397,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"a2plUHqB-pcy","outputId":"0afdffc5-045a-40ec-cfdf-a3de4b557bb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,626 B 0%] [Wa\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [2 InRelea\u001b[0m\u001b[33m\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","\u001b[33m\r0% [Waiting for headers] [3 InRelease 12.7 kB/110 kB 12%] [Waiting for headers]\u001b[0m\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\u001b[33m\r                                                                               \r0% [3 InRelease 14.2 kB/110 kB 13%] [Waiting for headers]\u001b[0m\r                                                         \rHit:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","\u001b[33m\r0% [Waiting for headers] [3 InRelease 14.2 kB/110 kB 13%]\u001b[0m\r                                                         \rHit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","\u001b[33m\r0% [Waiting for headers] [3 InRelease 17.1 kB/110 kB 15%]\u001b[0m\r                                                         \rGet:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","\u001b[33m\r0% [7 InRelease 5,484 B/119 kB 5%] [3 InRelease 22.9 kB/110 kB 21%] [Waiting fo\u001b[0m\r                                                                               \rHit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n","Fetched 7,496 kB in 3s (2,241 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","tar: spark-3.5.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=402b1285fac6c1545c06ebe989513ad005fd1a5f54b63d36927327c5f20bf6a3\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"]}],"source":["#!sudo apt update\n","#\n","#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#\n","#!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n","#!tar xf spark-3.5.0-bin-hadoop3.tgz\n","#\n","#!pip install -q findspark\n","#!pip install pyspark\n","#!pip install py4j"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4870,"status":"ok","timestamp":1715369747420,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"E5J9PtIx-qja"},"outputs":[],"source":["from ucimlrepo import fetch_ucirepo\n","\n","data = fetch_ucirepo(id=848)\n","\n","pdf = data.data.features\n","\n","pdf = pdf.drop(columns = [\n","    'cap-diameter',\n","    'stem-height',\n","    'stem-width',\n","])\n","\n","pdf = pdf.astype('<U22')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7109,"status":"ok","timestamp":1715369754527,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"gMNosrEl_PkH"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"K-Mode-global\") \\\n","    .getOrCreate()\n","\n","sc = spark.sparkContext"]},{"cell_type":"markdown","metadata":{},"source":["### Kmode sequential helper\n","I know that we should import module from another py file but this was developed on colab."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1715369754527,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"N079KTF1_Rf8"},"outputs":[],"source":["import random\n","import numpy as np\n","from collections import Counter\n","\n","class KMode:\n","\n","    def __init__(self, K, centroid=None) -> None:\n","        self.K = K\n","        self.centroid = centroid\n","\n","    def init_centroid(self, X):\n","        col_len = X.shape[1]\n","        centroid = np.array([], dtype=\"<U22\")\n","        for i in range(self.K):\n","            mode = []\n","            for col in range(col_len):\n","                rand_val = random.choice(np.unique(X[:, col]))\n","                mode.append(rand_val)\n","            if i == 0:\n","                centroid = np.array(mode, dtype=\"<U22\")\n","                del mode\n","            else:\n","                mode = np.array(mode, dtype=\"<U22\")\n","                centroid = np.vstack((centroid, mode))\n","        self.centroid = centroid\n","        return centroid\n","\n","    def get_closest_centroid(self, x):\n","        min_hamming_distance = np.inf\n","        closest_cluster = 0\n","        for i, mode in enumerate(self.centroid):\n","            distance = self.hamming_distance(x, mode)\n","            if distance < min_hamming_distance:\n","                min_hamming_distance = distance\n","                closest_cluster = i\n","        return closest_cluster\n","\n","    def fit(self, X, n_iters=100, stopping_criterion=1) -> None:\n","        losses = [] #for report\n","        if self.centroid is None:\n","          self.init_centroid(X)\n","        self.clustered = np.zeros(X.shape[0])\n","        for iter in range(n_iters):\n","            loss = self.fit_one_step(X)\n","            print(f\"iter: {iter} loss: {loss}\")\n","\n","            if loss < stopping_criterion:\n","                break\n","\n","    def fit_one_step(self, X) -> int:\n","        self.prev_centroid = self.centroid.copy()\n","        for i, x in enumerate(X):\n","            closest_centroid = self.get_closest_centroid(x)\n","            self.clustered[i] = closest_centroid\n","        for i in range(self.K):\n","            idx = np.where(self.clustered == i)[0]\n","            if idx.shape[0] > 0:\n","                new_mode = self.get_mode_from_arr(X[idx])\n","                self.centroid[i] = new_mode\n","        return self.hamming_distance(self.centroid, self.prev_centroid)\n","\n","    def transform(self, X_test):\n","        clustered = np.zeros(X_test.shape[0])\n","        for i, x in enumerate(X_test):\n","            closest_centroid = self.get_closest_centroid(x)\n","            clustered[i] = closest_centroid\n","        return clustered.astype('int')\n","\n","    @staticmethod\n","    def hamming_distance(a, b) -> int:\n","        return np.count_nonzero(a != b)\n","\n","    @staticmethod\n","    def get_mode_from_arr(arr):\n","        def get_mode_from_vec(vec):\n","            counted = Counter(vec)\n","            return counted.most_common(1)[0][0]\n","        P = arr.shape[1]\n","        mode_arr = np.full((arr.shape[1], ), \"\", dtype=\"<U22\")\n","        for p in range(P):\n","          mode = get_mode_from_vec(arr[:, p])\n","          mode_arr[p] = mode\n","        return mode_arr\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715369754527,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"dUDk3DZs_Xpr"},"outputs":[],"source":["def parition_to_numpy_array(instances):\n","  array = None\n","  for row in instances:\n","    if array is None:\n","      array = np.array(list(row.asDict().values()), dtype=\"<U22\")\n","    else:\n","      array_row = np.array(list(row.asDict().values()), dtype=\"<U22\")\n","      array = np.vstack((array, array_row))\n","  yield array"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715369754527,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"HTpjcG1a_bWc"},"outputs":[],"source":["def get_optimal_kmode_centroid(X, K=3):\n","  kmode = KMode(K)\n","  kmode.fit(X)\n","  return kmode.centroid"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27413,"status":"ok","timestamp":1715369781937,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"qdU_d43v_dHm","outputId":"c1baf596-ada2-4943-e42d-52a8fbdcf0b8"},"outputs":[{"data":{"text/plain":["MapPartitionsRDD[18] at coalesce at NativeMethodAccessorImpl.java:0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df = spark.createDataFrame(pdf)\n","df.cache()\n","rdd = df.rdd.repartition(10)\n","rdd.cache()"]},{"cell_type":"markdown","metadata":{},"source":["### Local approach"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167338,"status":"ok","timestamp":1715369997401,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"anEvUPTF_epf","outputId":"4460a92c-9c00-404f-ca53-ac91589d9e73"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0 loss: 23\n","iter: 1 loss: 4\n","iter: 2 loss: 0\n"]}],"source":["K = 3\n","\n","rdd = rdd.mapPartitions(parition_to_numpy_array)\n","### fit kmode on each partition seperately\n","rdd = rdd.map(lambda x: get_optimal_kmode_centroid(x, K))\n","### aggragrate centroid according to cluster\n","all_centroids = rdd.reduce(lambda x, y: np.vstack((x, y)))\n","### centroid now in driver. We then use a single model to fit on it\n","final_kmode = KMode(K)\n","final_kmode.fit(all_centroids)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1715369997402,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"RUq09sMn_f2y","outputId":"0b5ce57f-d744-42c8-c3be-b696ec3a74bf"},"outputs":[{"data":{"text/plain":["array([['x', 'nan', 'n', 'f', 'd', 'c', 'w', 'nan', 'nan', 'w', 'nan',\n","        'nan', 'f', 'f', 'nan', 'd', 'a'],\n","       ['x', 'nan', 'n', 'f', 'a', 'nan', 'w', 'nan', 'nan', 'w', 'nan',\n","        'nan', 'f', 'f', 'nan', 'd', 'a'],\n","       ['x', 'h', 'n', 'f', 'e', 'nan', 'w', 'nan', 'nan', 'w', 'nan',\n","        'nan', 't', 'e', 'nan', 'd', 'u']], dtype='<U22')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["final_kmode.centroid"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPonvkJB1bd9ZsHin1OMCYK","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
