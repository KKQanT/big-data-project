{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgvXYArKKRFr",
        "outputId": "8f9388e3-9dd2-4c9f-c07e-50e4768c35ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.6\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wgxGYY9KW71",
        "outputId": "60c1a68e-099c-46b4-e63e-9453d48d556a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/110 kB 13%] [Connect\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connected to ppa.\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connected to ppa.\u001b[0m\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Fetched 7,496 kB in 4s (1,992 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.5.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=539b69c3bb5f8786d96a4c3acc0f966d24d46a53a876b17d67a05793eac64851\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from math import e\n",
        "import random\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "def to_numpy(row):\n",
        "  a = list(row.asDict().values())\n",
        "  return np.array(a, dtype=\"<U22\")\n",
        "\n",
        "def hamming_distance(x1, x2):\n",
        "  return np.count_nonzero(x1!=x2)\n",
        "\n",
        "def get_closest_cluster_and_count(x, centroid):\n",
        "  min_hamming_distance = np.inf\n",
        "  closest_cluster = 0\n",
        "  for i, mode in enumerate(centroid):\n",
        "     distance = hamming_distance(x, mode)\n",
        "     if distance < min_hamming_distance:\n",
        "        min_hamming_distance = distance\n",
        "        closest_cluster = i\n",
        "\n",
        "  P = len(x)\n",
        "\n",
        "  count_elem_hash = {}\n",
        "  for i in range(P):\n",
        "    count_elem_hash[i] = {x[i] : 1}\n",
        "\n",
        "  return (closest_cluster, count_elem_hash)\n",
        "\n",
        "def merge_count_elem_hash(count_elem_hash_A, count_elem_hash_B):\n",
        "  for idx in count_elem_hash_A.keys():\n",
        "    for key in count_elem_hash_B[idx].keys():\n",
        "      if key in count_elem_hash_A[idx]:\n",
        "        count_elem_hash_A[idx][key] += count_elem_hash_B[idx][key]\n",
        "      else:\n",
        "        count_elem_hash_A[idx][key] =  count_elem_hash_B[idx][key]\n",
        "  return count_elem_hash_A\n",
        "\n",
        "\n",
        "def get_centroid(count_elem_hash):\n",
        "  P = len(count_elem_hash)\n",
        "  centroid = np.full((P,), \"\", dtype=\"<U22\")\n",
        "  for idx, count_hash in count_elem_hash.items():\n",
        "    mode = get_mode(count_hash)\n",
        "    centroid[idx] = mode\n",
        "  return centroid\n",
        "\n",
        "def get_mode(count_hash):\n",
        "  mode = \"\"\n",
        "  highest_count = 0\n",
        "  for value, count in count_hash.items():\n",
        "    if count > highest_count:\n",
        "      mode = value\n",
        "      highest_count = count\n",
        "  return mode"
      ],
      "metadata": {
        "id": "fjii9HeUKYhH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pdf = pd.read_csv('/content/drive/MyDrive/BigData/K-Mode/train.csv')\n",
        "pdf = pdf.drop(columns=['id', 'target'])\n",
        "pdf = pdf.astype('<U22')\n",
        "\n",
        "#from ucimlrepo import fetch_ucirepo\n",
        "#\n",
        "##\n",
        "#data = fetch_ucirepo(id=848)\n",
        "#pdf = data.data.features\n",
        "#\n",
        "#pdf = pdf.drop(columns = [\n",
        "#    'cap-diameter',\n",
        "#    'stem-height',\n",
        "#    'stem-width',\n",
        "#])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqp7cP7gLCEw",
        "outputId": "a06b8fbf-c5a9-4acc-e61e-7fe8a89804a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"K-Mode-global\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "TEb86roQK_vF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 3 #number of cluseters\n",
        "\n",
        "N_iters = 10\n",
        "\n",
        "N_partitions = 100\n",
        "\n",
        "P = len(pdf.columns)\n",
        "\n",
        "stop_distance = P//10"
      ],
      "metadata": {
        "id": "nprUVWwYKq3u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for frac in [0.25, 0.5, 0.75, 1]:\n",
        "  print(f'-------------------------------{frac}------------------------------')\n",
        "  df = spark.createDataFrame(pdf.sample(frac=frac).reset_index(drop=True))\n",
        "  df.cache()\n",
        "  rdd = df.rdd.repartition(N_partitions)\n",
        "  rdd.cache()\n",
        "\n",
        "  for (idx, row) in enumerate(rdd.takeSample(withReplacement=False, num=K, seed=42)):\n",
        "    if idx == 0:\n",
        "      init_centroid = np.array([to_numpy(row)]) #shape (1, P)\n",
        "    else:\n",
        "      c_ = to_numpy(row)\n",
        "      c_ = np.array([c_]) #shape (1, P)\n",
        "      init_centroid = np.concatenate([init_centroid, c_], axis=0)\n",
        "\n",
        "  centroid = sc.broadcast(init_centroid)\n",
        "\n",
        "  rdd = rdd.map(lambda row: to_numpy(row))\n",
        "\n",
        "  t_start = time.time()\n",
        "\n",
        "  for iter in range(N_iters):\n",
        "\n",
        "    clustered_and_hash_count_rdd = rdd.map(lambda x: get_closest_cluster_and_count(x, centroid.value)) #-> (k, v) = (cluster_i, count_hash)\n",
        "    counted_elem_rdd = clustered_and_hash_count_rdd.reduceByKey(lambda x, y: merge_count_elem_hash(x, y))\n",
        "    cluster_and_new_centroid_rdd =  counted_elem_rdd.map(lambda x: (x[0], get_centroid(x[1])))\n",
        "    centroid_hash_form = cluster_and_new_centroid_rdd.collect()\n",
        "\n",
        "    for idx in range(K):\n",
        "      if idx == 0:\n",
        "        new_centroid = np.array([centroid_hash_form[idx][1]])\n",
        "      else:\n",
        "        mode = np.array([centroid_hash_form[idx][1]])\n",
        "        new_centroid = np.concatenate([new_centroid, mode])\n",
        "\n",
        "    old_centroid = centroid.value.copy()\n",
        "    centroid = sc.broadcast(new_centroid)\n",
        "\n",
        "    distance = hamming_distance(old_centroid, new_centroid)\n",
        "\n",
        "    print('iteration : ', {iter+1}, \" hamming distance between new and previous centroid:  \", distance)\n",
        "\n",
        "    if distance <= stop_distance:\n",
        "      break\n",
        "\n",
        "  t_end = time.time()\n",
        "  print(f\"--- {t_end-t_start} seconds ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Dlhx-GLh11",
        "outputId": "4c078197-6686-4d1f-8250-9994417c9b89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------0.25------------------------------\n",
            "iteration :  {1}  hamming distance between new and previous centroid:   26\n",
            "iteration :  {2}  hamming distance between new and previous centroid:   0\n",
            "--- 93.15848207473755 seconds ---\n",
            "-------------------------------0.5------------------------------\n",
            "iteration :  {1}  hamming distance between new and previous centroid:   26\n",
            "iteration :  {2}  hamming distance between new and previous centroid:   0\n",
            "--- 112.63911437988281 seconds ---\n",
            "-------------------------------0.75------------------------------\n",
            "iteration :  {1}  hamming distance between new and previous centroid:   33\n",
            "iteration :  {2}  hamming distance between new and previous centroid:   0\n",
            "--- 145.37082600593567 seconds ---\n",
            "-------------------------------1------------------------------\n",
            "iteration :  {1}  hamming distance between new and previous centroid:   31\n",
            "iteration :  {2}  hamming distance between new and previous centroid:   0\n",
            "--- 154.88309502601624 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mushroom 848 - N_Partition = 10\n",
        "#-------------------------------0.25------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   9\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 10.779891729354858 seconds ---\n",
        "#-------------------------------0.5------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   12\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   2\n",
        "#iteration :  {3}  hamming distance between new and previous centroid:   2\n",
        "#iteration :  {4}  hamming distance between new and previous centroid:   1\n",
        "#--- 20.987311601638794 seconds ---\n",
        "#-------------------------------0.75------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   8\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 15.095524549484253 seconds ---\n",
        "#-------------------------------1------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   19\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 12.326220035552979 seconds ---\n",
        "\n",
        "\n",
        "#mushroom 848 - N_Partition = 50\n",
        "#-------------------------------0.25------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   16\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   2\n",
        "#iteration :  {3}  hamming distance between new and previous centroid:   0\n",
        "#--- 65.96836447715759 seconds ---\n",
        "#-------------------------------0.5------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   11\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 43.24885559082031 seconds ---\n",
        "#-------------------------------0.75------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   9\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 38.82074046134949 seconds ---\n",
        "#-------------------------------1------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   9\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 44.693320751190186 seconds --\n",
        "\n",
        "\n",
        "#mushroom 848 - N_Partition = 100\n",
        "#-------------------------------0.25------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   8\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 67.94282054901123 seconds ---\n",
        "#-------------------------------0.5------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   7\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 71.87999939918518 seconds ---\n",
        "#-------------------------------0.75------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   15\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 66.75218081474304 seconds ---\n",
        "#-------------------------------1------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   18\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 69.71056056022644 seconds ---"
      ],
      "metadata": {
        "id": "xDTFF3KQMlCh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kaggle  N_Partition = 10\n",
        "\n",
        "#-------------------------------0.25------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   31\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 27.516228437423706 seconds ---\n",
        "#-------------------------------0.5------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   27\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 50.537758111953735 seconds ---\n",
        "#-------------------------------0.75------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   33\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   2\n",
        "#--- 62.72692847251892 seconds ---\n",
        "#-------------------------------1------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   29\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 81.62160468101501 seconds ---\n",
        "\n",
        "#kaggle  N_Partition = 50\n",
        "\n",
        "#-------------------------------0.25------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   33\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 59.55855393409729 seconds ---\n",
        "#-------------------------------0.5------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   30\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   1\n",
        "#--- 79.15811371803284 seconds ---\n",
        "#-------------------------------0.75------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   29\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 98.91301417350769 seconds ---\n",
        "#-------------------------------1------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   31\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 113.1372766494751 seconds ---\n",
        "\n",
        "\n",
        "#kaggle N_partition = 100\n",
        "#-------------------------------0.25------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   26\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 93.15848207473755 seconds ---\n",
        "#-------------------------------0.5------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   26\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 112.63911437988281 seconds ---\n",
        "#-------------------------------0.75------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   33\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 145.37082600593567 seconds ---\n",
        "#-------------------------------1------------------------------\n",
        "#iteration :  {1}  hamming distance between new and previous centroid:   31\n",
        "#iteration :  {2}  hamming distance between new and previous centroid:   0\n",
        "#--- 154.88309502601624 seconds ---"
      ],
      "metadata": {
        "id": "03WjnN6loZJo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bC-Taaug8f-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}