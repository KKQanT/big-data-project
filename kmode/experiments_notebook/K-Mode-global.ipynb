{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "data = fetch_ucirepo(id=73)\n",
    "pdf = data.data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"K-Mode-global\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.cache()\n",
    "rdd = df.rdd.repartition(10)\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique value from each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "columns = df.columns\n",
    "unique_values_dict = {}\n",
    "for col in columns:\n",
    "  unique_val_objs = df.select(col).distinct().collect()\n",
    "  unique_val_list = [row[col] for row in unique_val_objs]\n",
    "  unique_values_dict[col] = unique_val_list\n",
    "  del unique_val_list\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ramdomly initialize centroid (Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "K = 5 #number of cluseters\n",
    "\n",
    "#count_row\n",
    "n_data = df.count()\n",
    "\n",
    "for (i, col) in enumerate(columns):\n",
    "  unique_values = unique_values_dict[col]\n",
    "  ramdom_vals = random.choices(unique_values, k=K)\n",
    "  if i == 0:\n",
    "    centroid = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
    "  else:\n",
    "    ramdom_vals = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
    "    centroid = np.hstack((centroid, ramdom_vals))\n",
    "\n",
    "centroid = sc.broadcast(centroid)\n",
    "centroid.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(row):\n",
    "  a = list(row.asDict().values())\n",
    "  return np.array(a, dtype=\"<U22\")\n",
    "\n",
    "rdd = rdd.map(lambda row: to_numpy(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def hamming_distance(x1, x2):\n",
    "  return np.count_nonzero(x1!=x2)\n",
    "\n",
    "def get_closest_cluster(x, centroid):\n",
    "  min_hamming_distance = np.inf\n",
    "  closest_cluster = 0\n",
    "  for i, mode in enumerate(centroid):\n",
    "     distance = hamming_distance(x, mode)\n",
    "     if distance < min_hamming_distance:\n",
    "        min_hamming_distance = distance\n",
    "        closest_cluster = i\n",
    "  return (closest_cluster, x)\n",
    "\n",
    "def get_mode_from_vec(vec):\n",
    "  counted = Counter(vec)\n",
    "  return counted.most_common(1)[0][0]\n",
    "\n",
    "def get_mode_from_arr(arr):\n",
    "  return np.apply_along_axis(get_mode_from_vec, 0, arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pararellize training phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_distance = 1\n",
    "\n",
    "N_iters = 10\n",
    "\n",
    "for iter in range(N_iters):\n",
    "  clustered = rdd.map(lambda x: get_closest_cluster(x, centroid.value)) #-> (k, v) = (cluster_i, X)\n",
    "  group_by_clustered = clustered.reduceByKey(lambda x, y: np.vstack((x, y)))\n",
    "  centroid_rdd = group_by_clustered.map(lambda x : (x[0], get_mode_from_arr(x[1])))\n",
    "  centroid_list = centroid_rdd.collect()\n",
    "\n",
    "  new_centroid = centroid.value.copy()\n",
    "  for (i, arr) in centroid_list:\n",
    "    new_centroid[i] = arr\n",
    "\n",
    "  old_centroid = centroid.value.copy()\n",
    "  centroid = sc.broadcast(new_centroid)\n",
    "\n",
    "  distance = hamming_distance(old_centroid, new_centroid)\n",
    "\n",
    "  print('iteration : ', {iter+1}, \" hamming distance between new and previous centroid:  \", distance)\n",
    "\n",
    "  if distance <= stop_distance:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
