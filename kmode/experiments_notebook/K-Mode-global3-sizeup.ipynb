{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23047,"status":"ok","timestamp":1715537869676,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"vgvXYArKKRFr","outputId":"6738232f-93fc-47c3-eb72-d504740f04de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ucimlrepo\n","  Downloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\n","Installing collected packages: ucimlrepo\n","Successfully installed ucimlrepo-0.0.6\n"]}],"source":["pip install ucimlrepo"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94312,"status":"ok","timestamp":1715537963985,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"2wgxGYY9KW71","outputId":"899b54c8-d044-49e2-8eea-fcd48b9b4f9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n","Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n","Fetched 7,542 kB in 3s (2,482 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","tar: spark-3.5.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=c5b56c52ef6111d0aea86112439ebbf9ea2f5d47591441d97294af229bd0792d\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"]}],"source":["!sudo apt update\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n","!tar xf spark-3.5.0-bin-hadoop3.tgz\n","\n","!pip install -q findspark\n","!pip install pyspark\n","!pip install py4j"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715542290847,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"fjii9HeUKYhH"},"outputs":[],"source":["import time\n","from math import e\n","import random\n","import numpy as np\n","import gc\n","\n","def to_numpy(row):\n","  a = list(row.asDict().values())\n","\n","  return np.array(a, dtype=\"<U22\")\n","\n","def hamming_distance(x1, x2):\n","  return np.count_nonzero(x1!=x2)\n","\n","def get_closest_cluster_and_count(x, centroid):\n","  min_hamming_distance = np.inf\n","  closest_cluster = 0\n","  for i, mode in enumerate(centroid):\n","     distance = hamming_distance(x, mode)\n","     if distance < min_hamming_distance:\n","        min_hamming_distance = distance\n","        closest_cluster = i\n","\n","  P = len(x)\n","\n","  count_elem_hash = {}\n","  for i in range(P):\n","    count_elem_hash[i] = {x[i] : 1}\n","\n","  return (closest_cluster, count_elem_hash)\n","\n","def merge_count_elem_hash(count_elem_hash_A, count_elem_hash_B):\n","  for idx in count_elem_hash_A.keys():\n","    for key in count_elem_hash_B[idx].keys():\n","      if key in count_elem_hash_A[idx]:\n","        count_elem_hash_A[idx][key] += count_elem_hash_B[idx][key]\n","      else:\n","        count_elem_hash_A[idx][key] =  count_elem_hash_B[idx][key]\n","  return count_elem_hash_A\n","\n","\n","def get_centroid(count_elem_hash):\n","  P = len(count_elem_hash)\n","  centroid = np.full((P,), \"\", dtype=\"<U22\")\n","  for idx, count_hash in count_elem_hash.items():\n","    mode = get_mode(count_hash)\n","    centroid[idx] = mode\n","  return centroid\n","\n","def get_mode(count_hash):\n","  mode = \"\"\n","  highest_count = 0\n","  for value, count in count_hash.items():\n","    if count > highest_count:\n","      mode = value\n","      highest_count = count\n","  return mode\n","\n","def merge_counts_within_partition(iterator):\n","    combined_counts = {}\n","    for x in iterator:\n","        cluster, count_hash = get_closest_cluster_and_count(x, centroid.value)\n","        if cluster in combined_counts:\n","            combined_counts[cluster] = merge_count_elem_hash(combined_counts[cluster], count_hash)\n","        else:\n","            combined_counts[cluster] = count_hash\n","    yield from combined_counts.items()"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1301,"status":"ok","timestamp":1715542292147,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"Qqp7cP7gLCEw"},"outputs":[],"source":["from ucimlrepo import fetch_ucirepo\n","\n","\n","data = fetch_ucirepo(id=848)\n","pdf = data.data.features\n","\n","pdf = pdf.drop(columns = [\n","    'cap-diameter',\n","    'stem-height',\n","    'stem-width',\n","])\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","#\n","#import pandas as pd\n","#\n","#pdf = pd.read_csv('/content/drive/MyDrive/BigData/K-Mode/train.csv')\n","#pdf = pdf.drop(columns=['id', 'target'])\n","#pdf = pdf.astype('<U22')\n","\n","\n","### 1M turkish market dataset\n","\n","#import pandas as pd\n","\n","#def categorize_price(x):\n","#  if x < 12.73:\n","#    return \"low\"\n","#  elif x < 30.62:\n","#    return \"medium\"\n","#  elif x < 67.7:\n","#    return \"high\"\n","#  return \"expensive\"\n","#\n","#pdf = pd.read_excel('<path to dataset>/kaggleSalesData.xlsx') #you can download this dataset from \n","#pdf = pdf[[\n","#    'PRICE',\n","#    'CATEGORY1',\n","#    'BRAND',\n","#    'USERGENDER',\n","#    'REGION'\n","#    ]]\n","#\n","#pdf['PRICE_CATEGORY'] = pdf['PRICE'].apply(lambda x: categorize_price(x))\n","#pdf = pdf.drop(columns = ['PRICE'])\n","#pdf = pdf.astype('<U22')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":14203,"status":"ok","timestamp":1715542306348,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"TEb86roQK_vF"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"K-Mode-global\") \\\n","    .getOrCreate()\n","\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":64657,"status":"ok","timestamp":1715542371003,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"nprUVWwYKq3u"},"outputs":[],"source":["K = 3 #number of cluseters\n","\n","N_iters = 10\n","\n","N_partitions = 100\n","\n","P = len(pdf.columns)\n","\n","stop_distance = P//10\n","\n","df = spark.createDataFrame(pdf)\n","df.cache()\n","rdd = df.rdd.repartition(N_partitions)\n","rdd.cache()\n","\n","for (idx, row) in enumerate(rdd.takeSample(withReplacement=False, num=K, seed=42)):\n","  if idx == 0:\n","    init_centroid = np.array([to_numpy(row)]) #shape (1, P)\n","  else:\n","    c_ = to_numpy(row)\n","    c_ = np.array([c_]) #shape (1, P)\n","    init_centroid = np.concatenate([init_centroid, c_], axis=0)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432032,"status":"ok","timestamp":1715542803026,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"06Dlhx-GLh11","outputId":"0d148914-59c0-4424-f3f7-33dfe3023155"},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 101.0346782207489 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 102.58198928833008 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 101.47393536567688 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 107.78599643707275 seconds ---\n"]}],"source":["for frac in [0.25, 0.5, 0.75, 1]:\n","  print(f'-------------------------------{frac}------------------------------')\n","  df = spark.createDataFrame(pdf.sample(frac=frac).reset_index(drop=True))\n","  df.cache()\n","  rdd = df.rdd.repartition(N_partitions)\n","  rdd.cache()\n","\n","  centroid = sc.broadcast(init_centroid)\n","\n","  rdd = rdd.map(lambda row: to_numpy(row))\n","\n","  t_start = time.time()\n","\n","  for iter in range(N_iters):\n","\n","    combined_counts_rdd = rdd.mapPartitions(merge_counts_within_partition)\n","    counted_elem_rdd = combined_counts_rdd.reduceByKey(lambda x, y: merge_count_elem_hash(x, y))\n","    cluster_and_new_centroid_rdd =  counted_elem_rdd.map(lambda x: (x[0], get_centroid(x[1])))\n","    centroid_hash_form = cluster_and_new_centroid_rdd.collect()\n","\n","    for idx in range(K):\n","      if idx == 0:\n","        new_centroid = np.array([centroid_hash_form[idx][1]])\n","      else:\n","        mode = np.array([centroid_hash_form[idx][1]])\n","        new_centroid = np.concatenate([new_centroid, mode])\n","\n","    old_centroid = centroid.value.copy()\n","    centroid = sc.broadcast(new_centroid)\n","\n","    distance = hamming_distance(old_centroid, new_centroid)\n","\n","    print('iteration : ', {iter+1}, \" hamming distance between new and previous centroid:  \", distance)\n","\n","    if distance <= stop_distance:\n","      break\n","\n","  t_end = time.time()\n","  print(f\"--- {t_end-t_start} seconds ---\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"elapsed":13,"status":"error","timestamp":1715542193111,"user":{"displayName":"Peerakarn Jitpukdee","userId":"02920075897590080466"},"user_tz":-60},"id":"xDTFF3KQMlCh","outputId":"ee766133-c1e9-4777-ee84-6638030b37f8"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-6-d5a1bddbd3ed>, line 3)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-d5a1bddbd3ed>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    -------------------------------0.25------------------------------\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["#mushroom 848 - N_Partition = 10\n","\n","-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   14\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 16.036723375320435 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   14\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 17.576813220977783 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   14\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 19.701108694076538 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   14\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 24.15873408317566 seconds ---\n","\n","#mushroom 848 - N_Partition = 50\n","#mushroom 848 - N_Partition = 50\n","-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   21\n","iteration :  {2}  hamming distance between new and previous centroid:   4\n","iteration :  {3}  hamming distance between new and previous centroid:   1\n","--- 56.45051574707031 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   20\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","iteration :  {3}  hamming distance between new and previous centroid:   1\n","--- 52.93846321105957 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   20\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","iteration :  {3}  hamming distance between new and previous centroid:   1\n","--- 59.03074359893799 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   19\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   1\n","--- 59.19886612892151 seconds ---\n","\n","\n","#mushroom 848 - N_Partition = 100\n","-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 101.0346782207489 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 102.58198928833008 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 101.47393536567688 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   15\n","iteration :  {2}  hamming distance between new and previous centroid:   3\n","iteration :  {3}  hamming distance between new and previous centroid:   0\n","--- 107.78599643707275 seconds ---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03WjnN6loZJo"},"outputs":[],"source":["#kaggle  N_Partition = 10\n","\n","-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   27\n","iteration :  {2}  hamming distance between new and previous centroid:   1\n","--- 40.02137732505798 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   27\n","iteration :  {2}  hamming distance between new and previous centroid:   1\n","--- 70.80644822120667 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   27\n","iteration :  {2}  hamming distance between new and previous centroid:   1\n","--- 95.41008138656616 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   27\n","iteration :  {2}  hamming distance between new and previous centroid:   1\n","--- 141.9313485622406 seconds ---\n","\n","#kaggle  N_Partition = 50\n","-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   36\n","iteration :  {2}  hamming distance between new and previous centroid:   0\n","--- 76.29413771629333 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   36\n","iteration :  {2}  hamming distance between new and previous centroid:   0\n","--- 105.43889665603638 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   36\n","iteration :  {2}  hamming distance between new and previous centroid:   0\n","--- 133.53474926948547 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   36\n","iteration :  {2}  hamming distance between new and previous centroid:   0\n","--- 184.66259908676147 seconds ---\n","\n","#kaggle N_partition = 100\n","-------------------------------0.25------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   30\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","--- 111.51788449287415 seconds ---\n","-------------------------------0.5------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   30\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","--- 143.34579515457153 seconds ---\n","-------------------------------0.75------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   30\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","--- 173.9554238319397 seconds ---\n","-------------------------------1------------------------------\n","iteration :  {1}  hamming distance between new and previous centroid:   30\n","iteration :  {2}  hamming distance between new and previous centroid:   2\n","--- 238.2033395767212 seconds ---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouT9T-ExDYWJ"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPA5pnsIvxKMnwbqg8Ufqzz","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
