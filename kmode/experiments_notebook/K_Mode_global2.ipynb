{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obdePh8bvOHp",
        "outputId": "238e4ca4-aad3-458c-f7c6-a5ba518c2a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from ucimlrepo import fetch_ucirepo\n",
        "#\n",
        "#data = fetch_ucirepo(id=76)\n",
        "#pdf = data.data.features\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pdf = pd.read_csv('data/train.csv')\n",
        "pdf = pdf.drop(columns=['id', 'target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qDcAejRAvPK3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"K-Mode-global\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwMfq4vhvaKn",
        "outputId": "f4aa0b91-db7d-4df3-936c-ce54e40a2617"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MapPartitionsRDD[18] at coalesce at NativeMethodAccessorImpl.java:0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark.createDataFrame(pdf)\n",
        "P = len(df.columns)\n",
        "df.cache()\n",
        "rdd = df.rdd.repartition(10)\n",
        "rdd.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j6C4b4wlvdwP"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hj4EoIrjvbF9"
      },
      "outputs": [],
      "source": [
        "def to_numpy(row):\n",
        "  a = list(row.asDict().values())\n",
        "  return np.array(a, dtype=\"<U22\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VtpAfTpvdKD",
        "outputId": "257ca500-9f37-44b9-ebc1-3a7ac904b4c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['0.0', '0.0', '0.0', 'F', 'Y', 'Blue', 'Circle', 'Hamster',\n",
              "        'Finland', 'Oboe', 'a256e9af4', '509d9d60b', '3600c6e91',\n",
              "        '5859a8a06', 'a02ae6a63', '1.0', 'Expert', 'Boiling Hot', 'b',\n",
              "        'P', 'hT', '6.0', '2.0'],\n",
              "       ['0.0', '0.0', '0.0', 'F', 'N', 'Blue', 'Trapezoid', 'Hamster',\n",
              "        'Costa Rica', 'Piano', 'eaf88078e', '16e480d8e', 'd40ca0718',\n",
              "        '153864851', '5533e4ecb', '1.0', 'Novice', 'Boiling Hot', 'h',\n",
              "        'R', 'DI', 'nan', '5.0'],\n",
              "       ['0.0', '0.0', '0.0', 'T', 'N', 'Blue', 'Circle', 'Hamster',\n",
              "        'China', 'Piano', 'e7eaa3efc', '6f800b9af', '86ec768cd',\n",
              "        '9fa5832d8', 'd7b264c91', '1.0', 'Master', 'Hot', 'g', 'P',\n",
              "        'NaN', '3.0', '9.0']], dtype='<U22')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from math import e\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "K = 3 #number of cluster\n",
        "\n",
        "for (idx, row) in enumerate(rdd.takeSample(withReplacement=False, num=K)):\n",
        "  if idx == 0:\n",
        "    centroid = np.array([to_numpy(row)]) #shape (1, P)\n",
        "  else:\n",
        "    c_ = to_numpy(row)\n",
        "    c_ = np.array([c_]) #shape (1, P)\n",
        "    centroid = np.concatenate([centroid, c_], axis=0)\n",
        "\n",
        "centroid = sc.broadcast(centroid) #shape (K, P)\n",
        "centroid.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E0LzTWxhFm_W"
      },
      "outputs": [],
      "source": [
        "#import gc\n",
        "#\n",
        "#columns = df.columns\n",
        "#unique_values_dict = {}\n",
        "#for col in columns:\n",
        "#  unique_val_objs = df.select(col).distinct().collect()\n",
        "#  unique_val_list = [row[col] for row in unique_val_objs]\n",
        "#  unique_values_dict[col] = unique_val_list\n",
        "#  del unique_val_list\n",
        "#  gc.collect()\n",
        "#\n",
        "#from math import e\n",
        "#import random\n",
        "#import numpy as np\n",
        "#\n",
        "#K = 3 #number of cluseters\n",
        "#\n",
        "##count_row\n",
        "#n_data = df.count()\n",
        "#\n",
        "#for (i, col) in enumerate(columns):\n",
        "#  unique_values = unique_values_dict[col]\n",
        "#  ramdom_vals = random.choices(unique_values, k=K)\n",
        "#  if i == 0:\n",
        "#    centroid = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
        "#  else:\n",
        "#    ramdom_vals = np.array(ramdom_vals).reshape(-1, 1).astype('str')\n",
        "#    centroid = np.hstack((centroid, ramdom_vals))\n",
        "#\n",
        "#centroid = sc.broadcast(centroid)\n",
        "#centroid.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oVJCp3SVvttx"
      },
      "outputs": [],
      "source": [
        "rdd = rdd.map(lambda row: to_numpy(row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4AmJGT26vuj8"
      },
      "outputs": [],
      "source": [
        "def hamming_distance(x1, x2):\n",
        "  return np.count_nonzero(x1!=x2)\n",
        "\n",
        "def get_closest_cluster_and_count(x, centroid):\n",
        "  min_hamming_distance = np.inf\n",
        "  closest_cluster = 0\n",
        "  for i, mode in enumerate(centroid):\n",
        "     distance = hamming_distance(x, mode)\n",
        "     if distance < min_hamming_distance:\n",
        "        min_hamming_distance = distance\n",
        "        closest_cluster = i\n",
        "\n",
        "  P = len(x)\n",
        "\n",
        "  count_elem_hash = {}\n",
        "  for i in range(P):\n",
        "    count_elem_hash[i] = {x[i] : 1}\n",
        "\n",
        "  return (closest_cluster, count_elem_hash)\n",
        "\n",
        "def merge_count_elem_hash(count_elem_hash_A, count_elem_hash_B):\n",
        "  for idx in count_elem_hash_A.keys():\n",
        "    for key in count_elem_hash_B[idx].keys():\n",
        "      if key in count_elem_hash_A[idx]:\n",
        "        count_elem_hash_A[idx][key] += count_elem_hash_B[idx][key]\n",
        "      else:\n",
        "        count_elem_hash_A[idx][key] = 1\n",
        "  return count_elem_hash_A\n",
        "\n",
        "def reduce_helper_count_elem(store_hash_map, x):\n",
        "  for i in range(len(x)):\n",
        "    if x[i] in store_hash_map[i]:\n",
        "      store_hash_map[i][x[i]] += 1\n",
        "    else:\n",
        "      store_hash_map[i][x[i]] = 1\n",
        "  return store_hash_map\n",
        "\n",
        "def get_centroid(count_elem_hash):\n",
        "  P = len(count_elem_hash)\n",
        "  centroid = np.full((P,), \"\")\n",
        "  for idx, count_hash in count_elem_hash.items():\n",
        "    mode = get_mode(count_hash)\n",
        "    centroid[idx] = mode\n",
        "  return centroid\n",
        "\n",
        "def get_mode(count_hash):\n",
        "  mode = \"\"\n",
        "  highest_count = 0\n",
        "  for value, count in count_hash.items():\n",
        "    if count > highest_count:\n",
        "      mode = value\n",
        "      highest_count = count\n",
        "  return mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKfwBeoKvwcs",
        "outputId": "ffd60fec-9351-4015-ac72-e99dec4de422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration :  {1}  hamming distance between new and previous centroid:   59\n",
            "iteration :  {2}  hamming distance between new and previous centroid:   7\n",
            "iteration :  {3}  hamming distance between new and previous centroid:   0\n"
          ]
        }
      ],
      "source": [
        "stop_distance = P//10\n",
        "\n",
        "N_iters = 10\n",
        "\n",
        "for iter in range(N_iters):\n",
        "\n",
        "  clustered_and_hash_count_rdd = rdd.map(lambda x: get_closest_cluster_and_count(x, centroid.value)) #-> (k, v) = (cluster_i, count_hash)\n",
        "  counted_elem_rdd = clustered_and_hash_count_rdd.reduceByKey(lambda x, y: merge_count_elem_hash(x, y))\n",
        "  cluster_and_new_centroid_rdd =  counted_elem_rdd.map(lambda x: (x[0], get_centroid(x[1])))\n",
        "  centroid_hash_form = cluster_and_new_centroid_rdd.collect()\n",
        "\n",
        "  for idx in range(K):\n",
        "    if idx == 0:\n",
        "      new_centroid = np.array([centroid_hash_form[idx][1]])\n",
        "    else:\n",
        "      mode = np.array([centroid_hash_form[idx][1]])\n",
        "      new_centroid = np.concatenate([new_centroid, mode])\n",
        "\n",
        "  old_centroid = centroid.value.copy()\n",
        "  centroid = sc.broadcast(new_centroid)\n",
        "\n",
        "  distance = hamming_distance(old_centroid, new_centroid)\n",
        "\n",
        "  print('iteration : ', {iter+1}, \" hamming distance between new and previous centroid:  \", distance)\n",
        "\n",
        "  if distance <= stop_distance:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ_KB3I9Imga"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
